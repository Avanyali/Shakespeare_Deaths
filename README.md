![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54) ![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=for-the-badge&logo=jupyter&logoColor=white) ![scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=for-the-badge&logo=scikit-learn&logoColor=white) ![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=for-the-badge&logo=pandas&logoColor=white) ![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white) ![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=for-the-badge&logo=Matplotlib&logoColor=black) ![Keras](https://img.shields.io/badge/Keras-%23D00000.svg?style=for-the-badge&logo=Keras&logoColor=white) ![TensorFlow](https://img.shields.io/badge/TensorFlow-%23FF6F00.svg?style=for-the-badge&logo=TensorFlow&logoColor=white)

The goal of this project was to create a model that would take in a corpus of character dialogue and predict whether the lived or died. For several reasons (humor, challenge, and copyright most prevalent) I chose to use the collected works of Shakespeare. The difficulty with this is that Shakespearean English is Early Modern English, which does not perfectly map onto what we speak and write today. This was partially solvable by digging into prebuilt language dictionaries and processing the text to replace words, but not entirely. Old styles of slang, conjugation, or old compound words weren't useful for the model. For these I created my own modernizing dictionaries until the language was all Modern English. 

After wrangling the data to be presentable, I wanted to get as much signal from it as possible. To do this, I added a number of NLP features to the dataframe during feature engineering: sentiment analysis to identify negativity or positivity on the part of the speaker, part of speech tagging to differentiate active an passive speakers, hypernyms to consolidate larger vocabularies into smaller and more meaningful ones, synonyms for the same reason as hypernyms, and lemmatizing to remove unnecessary conjugation (which was already accounted for partly by part of speech tagging). These were all chosen because using an unengineered bag-of-words would have taken both more processing power and had less predictive power. 

The last step was optimizing model predictiveness. ROC AUC was chosen as a metric because the goal was both to minimize false negatives and false positives, and separability of true positives and true negatives was desired for the predictive style of the model. The dataset still had two problems: one, the dataset was highly imbalanced between majority and minority class. SMOTE was used as a tool to created artifical data and even out the ratio. The second issue was that the number of features was still far too large for the processing capabilities of the server, and from early testing with subsets of the data many of those features were not relevant to the decision-making process. As inference was not required, PCA was used to reduced the feature set to a manageable level. From there, a fully-connected feed forward neural network was trained on the data testing combinations of early stopping, dropout layers, and regularization magnitude. The final model had an ROC AUC of 99.6%, and looking into feature selection by other white-box models suggests that it was not due to overpredictive features. 

A simple working widget can be run with streamlit and is in "notebooks/ShakespeareWidget.py". Character lines should be separated by newlines for best performance.
